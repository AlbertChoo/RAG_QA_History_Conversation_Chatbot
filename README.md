# ğŸ§  Conversational RAG with Chat History

A **Conversational Retrieval-Augmented Generation (RAG) pipeline** that enhances chatbot responses by retrieving relevant document context while maintaining conversation history.

## âœ¨ Features
âœ… **Conversational Memory** â€“ Maintains chat history for better context.  
âœ… **RAG-powered Retrieval** â€“ Retrieves top ğŸ” 3 relevant document chunks.  
âœ… **PDF Handling** â€“ Loads and processes documents efficiently.  
âœ… **Standalone Question Reformulation** â€“ Enhances user queries before retrieval.  
âœ… **Configurable LLM Response** â€“ Customizes chatbot answers based on prompts.  

---

## ğŸ› ï¸ Tech Stack
- **LLM Provider**: Groq API  
- **Vector Store**: ChromaDB  
- **Frontend**: Streamlit  
- **Embedding Model**: Sentence Transformers  
- **Orchestration**: LangChain  

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install Dependencies  
```bash
pip install -r requirements.txt
```
### 2ï¸âƒ£ Set Up API Key
Ensure you have a valid Groq API key. Create a .env file and add:
```env
GROQ_API_KEY="your_api_key_here"
```
Note: The .env file is not uploaded to GitHub (.gitignore includes it). Users must configure this themselves.
### 3ï¸âƒ£ Run the App
```bash
streamlit run app.py
```
Note: 'app.py' only process one PDF, where 'multiple-pdf-app.py' process multiple PDFs.
## Project Workflow
### ğŸ“‚ Document Processing
1ï¸âƒ£ Upload PDFs

2ï¸âƒ£ Save to Temporary Storage (temp_pdf/)

3ï¸âƒ£ Load, Split & Embed Documents

4ï¸âƒ£ Store in ChromaDB for Fast Retrieval

### ğŸ” Retrieval-Augmented Generation
1ï¸âƒ£ Retrieve Top k=3 Most Relevant Chunks

2ï¸âƒ£ Refine User Input into a Standalone Question

3ï¸âƒ£ Pass Reformulated Query to Retriever & LLM

4ï¸âƒ£ Generate an Answer Based on Retrieved Context

### ğŸ—£ï¸ Conversational Memory
1ï¸âƒ£ Stores chat history using RunnableWithMessageHistory

2ï¸âƒ£ Session-based retrieval to maintain conversation flow

3ï¸âƒ£ Improves chatbotâ€™s ability to understand multi-turn interactions

## âš™ï¸ Configuration & Customization
### ğŸ¯ Adjust Number of Retrieved Chunks
Modify k value to retrieve more/less context:
```python
retriever.search_kwargs["k"] = 3
```
### ğŸ“ Customize the QA Prompt
Modify qa_prompt to change response style.

### ğŸ”„ Enable Session-based Memory
Chat history is maintained using:
```python
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain, get_session_history(session_id),
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer"
)
```
## ğŸ—ï¸ Project Setup Notes
The .gitignore file ensures that the venv (virtual environment) and .env file are not uploaded to GitHub.

Users need to set up their own virtual environment and configure the API key before running the app.
## ğŸ† Future Improvements
ğŸ”¹ Fine-tune the LLM for better domain-specific responses

ğŸ”¹ Improve retrieval with hybrid search (BM25 + Embeddings)

ğŸ”¹ Optimize query reformulation using advanced LLM prompts
## ğŸ¤ Contributing
Got an idea? Feel free to open an issue or submit a PR! ğŸš€

## ğŸ“œ License
This project is licensed under the MIT License.
